{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, random, math, string\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17765420</th>\n",
       "      <td>Operating in the rugged Arctic always poses un...</td>\n",
       "      <td>Les conditions rudes de l’Arctique ont toujour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7566010</th>\n",
       "      <td>Building a Knowledge Culture</td>\n",
       "      <td>Création d'une culture du savoir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13382656</th>\n",
       "      <td>More than 360 current and former Cadets of the...</td>\n",
       "      <td>Plus de 360 anciens et actuels de divers campu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19904187</th>\n",
       "      <td>Like the Château Saint-Louis it was a 2-storey...</td>\n",
       "      <td>Comme le château Saint-Louis, il s'agit d'un é...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12978711</th>\n",
       "      <td>• L'administration québécoise trailed the Queb...</td>\n",
       "      <td>Pour les entreprises dans le secteur privé don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6369958</th>\n",
       "      <td>5.1 Most of our larger companies now have cons...</td>\n",
       "      <td>5.1 La plupart de nos grandes entreprises ont ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15684999</th>\n",
       "      <td>However, there is a reporting relationship ins...</td>\n",
       "      <td>Toutefois, il existe tout de même un lien hiér...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425701</th>\n",
       "      <td>A draft Institute evaluation report will be su...</td>\n",
       "      <td>Les consultants soumettront un rapport proviso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19697524</th>\n",
       "      <td>This occurred during th Management Simulation ...</td>\n",
       "      <td>Cela s'est produit durant l'exercice de simula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16381192</th>\n",
       "      <td>If this is not possible, please provide the ge...</td>\n",
       "      <td>Si cela n’est pas possible, veuillez fournir l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         en  \\\n",
       "17765420  Operating in the rugged Arctic always poses un...   \n",
       "7566010                        Building a Knowledge Culture   \n",
       "13382656  More than 360 current and former Cadets of the...   \n",
       "19904187  Like the Château Saint-Louis it was a 2-storey...   \n",
       "12978711  • L'administration québécoise trailed the Queb...   \n",
       "...                                                     ...   \n",
       "6369958   5.1 Most of our larger companies now have cons...   \n",
       "15684999  However, there is a reporting relationship ins...   \n",
       "1425701   A draft Institute evaluation report will be su...   \n",
       "19697524  This occurred during th Management Simulation ...   \n",
       "16381192  If this is not possible, please provide the ge...   \n",
       "\n",
       "                                                         fr  \n",
       "17765420  Les conditions rudes de l’Arctique ont toujour...  \n",
       "7566010                    Création d'une culture du savoir  \n",
       "13382656  Plus de 360 anciens et actuels de divers campu...  \n",
       "19904187  Comme le château Saint-Louis, il s'agit d'un é...  \n",
       "12978711  Pour les entreprises dans le secteur privé don...  \n",
       "...                                                     ...  \n",
       "6369958   5.1 La plupart de nos grandes entreprises ont ...  \n",
       "15684999  Toutefois, il existe tout de même un lien hiér...  \n",
       "1425701   Les consultants soumettront un rapport proviso...  \n",
       "19697524  Cela s'est produit durant l'exercice de simula...  \n",
       "16381192  Si cela n’est pas possible, veuillez fournir l...  \n",
       "\n",
       "[500000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"D:/AI/S1_Y3/DL/Code/seminar/en-fr.csv\")\n",
    "data = data.sample(n=500000, random_state=42)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 2), (25000, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, temp = train_test_split(data, test_size=0.1, random_state=42) \n",
    "train_data.shape, temp.shape\n",
    "valid_data, test_data = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "valid_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.to_dict(orient='records')\n",
    "valid_data=valid_data.to_dict(orient='records')\n",
    "test_data=test_data.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {key: [d[key] for d in train_data] for key in train_data[0]}\n",
    "train_data = Dataset.from_dict(train_dict)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'fr'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dict = {key: [d[key] for d in valid_data] for key in valid_data[0]}\n",
    "valid_data = Dataset.from_dict(valid_dict)\n",
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'fr'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict = {key: [d[key] for d in test_data] for key in test_data[0]}\n",
    "test_data = Dataset.from_dict(test_dict)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 2.1/12.8 MB 7.3 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.2/12.8 MB 8.4 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.8/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 8.2 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/12.8 MB 8.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 8.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 8.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 7.8 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "     ---------------------------------------- 0.0/16.3 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/16.3 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.6/16.3 MB 6.4 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.7/16.3 MB 8.4 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 5.2/16.3 MB 8.2 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 7.1/16.3 MB 8.2 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 8.7/16.3 MB 8.3 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 9.7/16.3 MB 7.8 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 10.7/16.3 MB 7.5 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 11.8/16.3 MB 7.0 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 12.6/16.3 MB 6.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 13.4/16.3 MB 6.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 14.4/16.3 MB 6.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 15.2/16.3 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  16.3/16.3 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 16.3/16.3 MB 6.0 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "fr_nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example, en_nlp, fr_nlp, max_length, lower, sos_token, eos_token):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    fr_tokens = [token.text for token in fr_nlp.tokenizer(example[\"fr\"])][:max_length]\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "        fr_tokens = [token.lower() for token in fr_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    fr_tokens = [sos_token] + fr_tokens + [eos_token]\n",
    "    return {\"en_tokens\": en_tokens, \"fr_tokens\": fr_tokens}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81e8a19f5b74e068098cf6a8c50a253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/450000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c87bd520ecf4368ae65cb202d1150ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1397b6c9abbd4b18ae92d5a43d591566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 1_000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"fr_nlp\": fr_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "fr_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"fr_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert en_vocab[unk_token] == fr_vocab[unk_token]\n",
    "assert en_vocab[pad_token] == fr_vocab[pad_token]\n",
    "\n",
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab.set_default_index(unk_index)\n",
    "fr_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(example, en_vocab, fr_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    fr_ids = fr_vocab.lookup_indices(example[\"fr_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"fr_ids\": fr_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_kwargs = {\"en_vocab\": en_vocab, \"fr_vocab\": fr_vocab}\n",
    "\n",
    "train_data = train_data.map(numericalize, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_data)\n",
    "valid_df = pd.DataFrame(valid_data)\n",
    "test_df = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"fr_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_fr_ids = [example[\"fr_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_fr_ids = nn.utils.rnn.pad_sequence(batch_fr_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"fr_ids\": batch_fr_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        # embedded : [sen_len, batch_size, emb_dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=self.n_layers, dropout=dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        # input : [1, ,batch_size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch_size, emb_dim]\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch_size, output_dim]\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            'hidden dimensions of encoder and decoder must be equal.'\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            'n_layers of encoder and decoder must be equal.'\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src = [sen_len, batch_size]\n",
    "        # trg = [sen_len, batch_size]\n",
    "        # teacher_forcing_ratio : the probability to use the teacher forcing.\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # first input to the decoder is the <sos> token.\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_len):\n",
    "            # insert input token embedding, previous hidden and previous cell states \n",
    "            # receive output tensor (predictions) and new hidden and cell states.\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            # replace predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # decide if we are going to use teacher forcing or not.\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # get the highest predicted token from our predictions.\n",
    "            top1 = output.argmax(1)\n",
    "            # update input : use ground_truth when teacher_force \n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SRC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# First initialize our model.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m INPUT_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mSRC\u001b[49m\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[0;32m      3\u001b[0m OUTPUT_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(TRG\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[0;32m      4\u001b[0m ENC_EMB_DIM \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SRC' is not defined"
     ]
    }
   ],
   "source": [
    "input_dim = len(fr_vocab)\n",
    "output_dim = len(en_vocab)\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "hidden_dim = 1024\n",
    "n_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    decoder_dropout,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model, device_ids=[0,1]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7805, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5940, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=5940, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 13,922,356 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # trg = [sen_len, batch_size]\n",
    "        # output = [trg_len, batch_size, output_dim]\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        # transfrom our output : slice off the first column, and flatten the output into 2 dim.\n",
    "        output = output[1:].view(-1, output_dim) \n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(trg_len-1) * batch_size]\n",
    "        # output = [(trg_len-1) * batch_size, output_dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            output = model(src, trg, 0) # turn off teacher forcing.\n",
    "            \n",
    "            # trg = [sen_len, batch_size]\n",
    "            # output = [sen_len, batch_size, output_dim]\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that used to tell us how long an epoch takes.\n",
    "def epoch_time(start_time, end_time):\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time  / 60)\n",
    "    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n",
    "    return  elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time 0m 36s\n",
      "\tTrain Loss: 5.209 | Train PPL: 182.921\n",
      "\tValid Loss: 5.134 | Valid PPL: 169.759\n",
      "Epoch: 02 | Time 0m 36s\n",
      "\tTrain Loss: 4.716 | Train PPL: 111.673\n",
      "\tValid Loss: 5.078 | Valid PPL: 160.428\n",
      "Epoch: 03 | Time 0m 36s\n",
      "\tTrain Loss: 4.392 | Train PPL:  80.804\n",
      "\tValid Loss: 4.703 | Valid PPL: 110.315\n",
      "Epoch: 04 | Time 0m 35s\n",
      "\tTrain Loss: 4.121 | Train PPL:  61.630\n",
      "\tValid Loss: 4.535 | Valid PPL:  93.258\n",
      "Epoch: 05 | Time 0m 35s\n",
      "\tTrain Loss: 3.937 | Train PPL:  51.253\n",
      "\tValid Loss: 4.378 | Valid PPL:  79.707\n",
      "Epoch: 06 | Time 0m 35s\n",
      "\tTrain Loss: 3.763 | Train PPL:  43.071\n",
      "\tValid Loss: 4.323 | Valid PPL:  75.403\n",
      "Epoch: 07 | Time 0m 35s\n",
      "\tTrain Loss: 3.587 | Train PPL:  36.115\n",
      "\tValid Loss: 4.214 | Valid PPL:  67.598\n",
      "Epoch: 08 | Time 0m 36s\n",
      "\tTrain Loss: 3.436 | Train PPL:  31.073\n",
      "\tValid Loss: 4.110 | Valid PPL:  60.942\n",
      "Epoch: 09 | Time 0m 35s\n",
      "\tTrain Loss: 3.330 | Train PPL:  27.934\n",
      "\tValid Loss: 3.976 | Valid PPL:  53.301\n",
      "Epoch: 10 | Time 0m 35s\n",
      "\tTrain Loss: 3.177 | Train PPL:  23.976\n",
      "\tValid Loss: 3.910 | Valid PPL:  49.902\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    valid_loss = evaluate(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"translation-model.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 3.917 | Test PPL:  50.228\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    best_model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "    best_model.load_state_dict(torch.load('Seq2SeqModel.pt'))\n",
    "    \n",
    "    test_loss = evaluate(model, test_iter, criterion)\n",
    "    \n",
    "    print(f\"Test Loss : {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f}\")\n",
    "    \n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
